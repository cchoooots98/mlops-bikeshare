name: CD_Staging

on:
  push:
    branches: ["main"]
  workflow_dispath:

env:
  AWS_REGION: ${{ vars.AWS_REGION }}            
  ENDPOINT_NAME: ${{ vars.ENDPOINT_PROD }}      
  INSTANCE_TYPE: ${{ vars.INSTANCE_PROD }}      
  ECR_IMAGE_URI: ${{ vars.ECR_IMAGE_URI }}      
  DEFAULT_S3_MODEL_TAR: ${{ vars.S3_MODEL_TAR }}
  SM_EXECUTION_ROLE_Arn: ${{ secrets.SM_EXECUTION_ROLE_Arn }}

permissions:
  id_token: write
  contents: read

concurrency:
  group: staging-deploy
  cancel-in-progress: false  # Avoid interrupting a live rollout

jobs:
  deploy-staging:
    name: Deploy to SageMaker (staging)
    runs-on: ubuntu-latest
    environment: staging

    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Configure AWS (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ env.AWS_REGION }}
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          role-session-name: gh-actions-prod

      - name: Resolve model artifact
        id: resolve
        run: |
          # If user passed override_model_tar, use it; else use DEFAULT_S3_MODEL_TAR
          if [ -n "${{ github.event.inputs.override_model_tar }}" ]; then
            echo "model_tar=${{ github.event.inputs.override_model_tar }}" >> "$GITHUB_OUTPUT"
          else
            echo "model_tar=${DEFAULT_S3_MODEL_TAR}" >> "$GITHUB_OUTPUT"
          fi

      # OPTIONAL: If you routinely rebuild/push the BYOC image, add ECR login + docker build/push here.

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install deploy deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Capture previous endpoint config (for rollback)
        id: prev
        shell: bash
        run: |
          set -euo pipefail
          PREV_CFG=$(aws sagemaker describe-endpoint \
            --endpoint-name "${ENDPOINT_NAME}" \
            --query 'EndpointConfigName' --output text 2>/dev/null || echo "NONE")
          echo "prev_config=$PREV_CFG" >> "$GITHUB_OUTPUT"

      - name: Deploy new config (blue/green)
        id: deploy
        run: |
          python pipelines/deploy_via_sagemaker_sdk.py \
            --endpoint-name "${ENDPOINT_NAME}" \
            --role-arn "${SM_EXECUTION_ROLE_Arn}" \
            --image-uri "${ECR_IMAGE_URI}" \
            --model-data "${{ steps.resolve.outputs.model_tar }}" \
            --instance-type "${INSTANCE_TYPE}" \
            --region "${AWS_REGION}"

      - name: Smoke test prod (use repo script to match schema exactly)
        shell: bash
        run: |
          # Install deps if your requirements.txt doesn't already include boto3
          # English: Ensure boto3 is available for the script that invokes the endpoint.
          python -m pip install --upgrade pip
          pip install -r requirements.txt || true
          python - <<'PY'
          import importlib.util, sys
          # English: Fallback install if boto3 not included in requirements.
          spec = importlib.util.find_spec("boto3")
          print("boto3 found:", bool(spec))
          if not spec:
              import subprocess
              subprocess.check_call([sys.executable, "-m", "pip", "install", "boto3"])
          PY
          
          python test/smoke_invoke.py --endpoint-name "${ENDPOINT_NAME}" --region "${AWS_REGION}"


      - name: Rollback on failure
        if: failure() && steps.prev.outputs.prev_config != 'NONE'
        run: |
          echo "Smoke test failed â€” rolling back."
          aws sagemaker update-endpoint \
            --endpoint-name "${ENDPOINT_NAME}" \
            --endpoint-config-name "${{ steps.prev.outputs.prev_config }}" \
            --region "${AWS_REGION}"